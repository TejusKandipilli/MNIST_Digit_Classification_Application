{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a66567-86a3-494b-ab07-9ce683d58387",
   "metadata": {},
   "source": [
    "# MNIST Digit Classification Application with Convolutional Neural Network\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94398ecc-601d-46b4-809f-f8e981fc8a0d",
   "metadata": {},
   "source": [
    "In the ever-evolving landscape of artificial intelligence and machine learning, the ability to effectively recognize and classify patterns in data is a fundamental aspect. One powerful approach for image classification tasks is the utilization of Convolutional Neural Networks (CNNs). This project embarks on a journey to build and train a sophisticated image classification model using the renowned deep learning library, **Keras**, complemented by the essential tools **Matplotlib** and **Numpy**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe620e9f-1178-4cf3-9a4c-0aee4d6ba83e",
   "metadata": {},
   "source": [
    "## Objective\r\n",
    "\r\n",
    "The primary goal of this project is to construct a robust Convolutional Neural Network (CNN) specifically designed for the classification of MNIST handwritten numerical digits. Leveraging Keras, a high-level neural networks API, provides an intuitive and efficient platform for developing and training deep learning models tailored for this task. The inclusion of Matplotlib facilitates visualizations, enabling a comprehensive understanding of the model's performance on the MNIST dataset, while Numpy supports efficient numerical operations essential for data manipulation in the context of digit classification.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b203e017-fde3-4211-bdd5-0a0243cf395d",
   "metadata": {},
   "source": [
    "## Key Components\n",
    "1. **Keras:** As the core framework, Keras streamlines the implementation of neural networks, abstracting complexities and providing a user-friendly interface. The project delves into the sequential construction of a CNN, incorporating convolutional and pooling layers, flattening operations, and fully connected layers.\n",
    "\n",
    "2. **Matplotlib:** Visualizing the model's training progress and performance is crucial for insights. Matplotlib comes into play for generating informative plots and graphs, allowing stakeholders to interpret accuracy trends, loss convergence, and other crucial metrics.\n",
    "\n",
    "3. **Numpy:** A fundamental library for numerical operations, Numpy facilitates efficient data manipulation and array processing. It is instrumental in reshaping input data, handling mathematical computations, and ensuring seamless integration with Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546fec5a-600a-4478-b996-f357f210a563",
   "metadata": {},
   "source": [
    "## Project Workflow\n",
    "1. **Data Loading and Exploration**\n",
    "2. **Data Preprocessing**\n",
    "3. **Model Architecture**\n",
    "4. **Compilation**\n",
    "5. **Training**\n",
    "6. **Application**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b9fc2-fc52-4f65-9889-4fc0b7061210",
   "metadata": {},
   "source": [
    "## Code Implementation\n",
    "\n",
    "Now that we've outlined the objectives, key components, and workflow of our machine learning project, let's dive into the practical aspect. The following code implementation demonstrates the step-by-step process of constructing and training a Convolutional Neural Network (CNN) using the powerful tools at our disposal: **Keras** for model development, **Matplotlib** for insightful visualizations, and **Numpy** for efficient numerical operations.\n",
    "\n",
    "In the subsequent sections, we'll walk through:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe03f3-5d37-4f34-be93-81be6e7f4168",
   "metadata": {},
   "source": [
    "### 1.Data Loading and Exploration \n",
    "Understanding the structure of our data is crucial. We'll explore the MNIST dataset to gain insights into its dimensions and characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42aa4cb-9d4c-4c83-8833-90e0808a6c57",
   "metadata": {},
   "source": [
    "**Importing Essential Libraries**\r\n",
    "\r\n",
    "In this section, we import the necessary libraries essential for the implementation:\r\n",
    "\r\n",
    "- **numpy:** A library for numerical operations in Python.\r\n",
    "- **matplotlib.pyplot:** A library for creating visualizations in Python.\r\n",
    "- **to_categorical:** A function from Keras utilities to convert integer labels to one-hot encoding.\r\n",
    "- **Sequential:** A Keras class for creating a linear stack of layers in a neural network.\r\n",
    "- **Conv2D, MaxPooling2D, Flatten, Dense, Dropout:** Different layers used in building a Convolutional Neural Network (CNN).\r\n",
    "- **EarlyStopping, ModelCheckpoint:** Callbacks from Keras for early stopping during training and model checkpointing.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1f24edb-2c62-4700-b8e5-8d7f7b08eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.datasets import mnist\n",
    "from tensorflow.compat.v1.train import AdamOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb18d3d-dd09-4e67-bc13-5b9fcdc3ca3c",
   "metadata": {},
   "source": [
    "### 2.Data Preprocessing: \n",
    "Before feeding the data into the model, we'll perform necessary preprocessing steps, including converting labels to one-hot encoding and reshaping the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9252050-3e5b-42a0-8c5c-91977f589303",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baee049-f3cb-4efb-9e66-2ada473ed6e9",
   "metadata": {},
   "source": [
    "This single line of code above fetches the MNIST dataset, splitting it into training and testing sets. It assigns the images of the training set to `X_train` and the corresponding labels to `y_train`. Similarly, it assigns the images of the testing set to `X_test` and the corresponding labels to `y_test`.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec587ec4-8504-48fa-b86c-c1a8c7e4ef05",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To gain insights into the structure of the dataset, we print the shapes of the training and testing data arrays using the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7114152-b45d-43a0-b827-f597c222f57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 28, 28), Labels shape: (60000,)\n",
      "Testing data shape: (10000, 28, 28), Labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data shape: {X_train.shape}, Labels shape: {y_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}, Labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e31d7c7-efa8-4550-ac73-f88ed73fd3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=10)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef08055-9911-4a85-b127-afc156dbcd5d",
   "metadata": {},
   "source": [
    "\n",
    "Here, the integer labels in y_train are converted to one-hot encoding using the to_categorical function. Additionally, the input data (X_train) is reshaped to include a channel dimension, which is necessary for CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5388c38-efed-4752-acdc-009070b53cb6",
   "metadata": {},
   "source": [
    "### 3.Model Architecture\n",
    "The heart of our project lies in designing a robust CNN. We'll sequentially add convolutional layers, pooling, flattening operations, and dense layers to form a powerful architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1f57c80-5cd2-4bec-ba8e-527a370d57af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(28, 28, 1), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3695b4b4-08fc-41b4-9715-c174eefaec71",
   "metadata": {},
   "source": [
    "The architecture includes the following layers:\r\n",
    "\r\n",
    "1. **Conv2D:** Convolutional layer with 32 filters, each filter scanning the input image with a kernel size of (3, 3). The ReLU activation function is applied, introducing non-linearity to the model. This layer is responsible for capturing local patterns in the input images.\r\n",
    "\r\n",
    "2. **MaxPooling2D:** Max-pooling layer with a pool size of (2, 2). It reduces the spatial dimensions of the representation and retains the most important information, providing translation invariance. This helps in reducing computation and controlling overfitting.\r\n",
    "\r\n",
    "3. **Conv2D:** Another convolutional layer with 64 filters, similar to the first Conv2D layer. This layer further refines the learned features from the previous layer, extracting more complex and abstract features.\r\n",
    "\r\n",
    "4. **MaxPooling2D:** Another max-pooling layer to downsample the spatial dimensions further. It maintains the most relevant information while reducing computational complexity.\r\n",
    "\r\n",
    "5. **Flatten:** Flattening layer that converts the 2D matrix data to a vector. It prepares the data for input into the fully connected layers.\r\n",
    "\r\n",
    "6. **Dropout:** Dropout layer with a rate of 0.25. It randomly drops a fraction of input units during training, acting as a regularization technique to prevent overfitting.\r\n",
    "\r\n",
    "7. **Dense:** Fully connected layer with 10 units, corresponding to the 10 classes of digits (0-9). The softmax activation function is applied, providing probabilities for each class. This layer produces the final output for digit classification.\r\n",
    "\r\n",
    "This architecture is specifically tailored for the MNIST digit classification task. Each layer plays a crucial role in capturing hierarchical features from the input images, reducing spatial dimensions, and making predictions. The Convolutional Neural Network's ability to automatically learn and extract relevant features makes it well-suited for image classification tasks.\r\n",
    " Network.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35efdb9-2b50-49bf-b51a-2940413489d8",
   "metadata": {},
   "source": [
    "### 4.Compilation\n",
    "Configuring the model involves specifying an optimizer, a loss function, and evaluation metrics. We'll compile our CNN to prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71c630fc-3e1b-48ee-be8a-7815c9edff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=AdamOptimizer(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a8bb04-875b-4884-be60-2f848f9f8fb2",
   "metadata": {},
   "source": [
    "**In this compilation step:**\r\n",
    "\r\n",
    "- **Optimizer ('adam'):** The Adam optimizer is employed, representing a widely used and efficient optimization algorithm for training neural networks. It adapts learning rates for each parameter individually, enhancing the efficiency of the training process. Adam is known for its robustness and effectiveness in a variety of scenarios.\r\n",
    "\r\n",
    "- **Loss Function ('categorical_crossentropy'):** This is the chosen loss function for multiclass classification tasks, such as MNIST digit classification. Categorical crossentropy measures the difference between the predicted probability distribution and the true distribution of the labels. It quantifies how well the model is aligning its predictions with the actual class labels.\r\n",
    "\r\n",
    "- **Metric to Monitor ('accuracy'):** The accuracy metric is selected to monitor the model's performance during training. Accuracy provides a straightforward measure of how well the model is classifying the input images, representing the ratio of correctly predicted samples to the total number of samples. Monitoring accuracy helps ensure that the model is making meaningful progress in learning and generalizing patterns from the training data.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f766118-a4dd-4307-b6b1-735183970dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=4, verbose=1, restore_best_weights=True)\n",
    "mc = ModelCheckpoint(\"best.keras\", monitor=\"val_accuracy\", verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e8083-060c-4bef-ba76-760d45d8448e",
   "metadata": {},
   "source": [
    "**Callbacks are defined to enhance training:**\n",
    "\n",
    "**EarlyStopping**:\n",
    "- **Purpose:** Monitors the validation accuracy during training.\n",
    "- **Functionality:** Stops training if there's no improvement after a certain number of epochs (patience).\n",
    "- **Action on Stop:** Restores the model weights to the best weights recorded during training.\n",
    "\n",
    "**ModelCheckpoint**:\n",
    "- **Purpose:** Monitors the validation accuracy during training.\n",
    "- **Functionality:** Saves the best model based on validation accuracy.\n",
    "ccuracy.\r\n",
    "ccuracy.\r\n",
    "cy.\r\n",
    "ng."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe5d90-396b-4250-bcd2-b1196ba111f0",
   "metadata": {},
   "source": [
    "### 5.Training\n",
    "The hands-on training process begins, with a focus on performance metrics. We'll monitor key indicators and use early stopping and model checkpointing for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e7b3a86-6cb8-4dd7-a6ca-a1c805e24101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0922 - accuracy: 0.9720 - val_loss: 0.0638 - val_accuracy: 0.9810\n",
      "Epoch 2/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0758 - accuracy: 0.9776 - val_loss: 0.0595 - val_accuracy: 0.9819\n",
      "Epoch 3/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0692 - accuracy: 0.9791 - val_loss: 0.0753 - val_accuracy: 0.9809\n",
      "Epoch 4/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0639 - accuracy: 0.9798 - val_loss: 0.0859 - val_accuracy: 0.9772\n",
      "Epoch 5/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0594 - accuracy: 0.9824 - val_loss: 0.0574 - val_accuracy: 0.9839\n",
      "Epoch 6/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0561 - accuracy: 0.9831 - val_loss: 0.0607 - val_accuracy: 0.9850\n",
      "Epoch 7/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0498 - accuracy: 0.9849 - val_loss: 0.0589 - val_accuracy: 0.9841\n",
      "Epoch 8/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0453 - accuracy: 0.9868 - val_loss: 0.0703 - val_accuracy: 0.9831\n",
      "Epoch 9/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0471 - accuracy: 0.9860 - val_loss: 0.0656 - val_accuracy: 0.9844\n",
      "Epoch 10/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0434 - accuracy: 0.9874 - val_loss: 0.0597 - val_accuracy: 0.9862\n",
      "Epoch 11/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0411 - accuracy: 0.9875 - val_loss: 0.0676 - val_accuracy: 0.9843\n",
      "Epoch 12/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0396 - accuracy: 0.9891 - val_loss: 0.0714 - val_accuracy: 0.9853\n",
      "Epoch 13/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0432 - accuracy: 0.9885 - val_loss: 0.0639 - val_accuracy: 0.9867\n",
      "Epoch 14/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0376 - accuracy: 0.9899 - val_loss: 0.0890 - val_accuracy: 0.9839\n",
      "Epoch 15/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0457 - accuracy: 0.9885 - val_loss: 0.0828 - val_accuracy: 0.9867\n",
      "Epoch 16/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0387 - accuracy: 0.9903 - val_loss: 0.0838 - val_accuracy: 0.9858\n",
      "Epoch 17/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0405 - accuracy: 0.9894 - val_loss: 0.0842 - val_accuracy: 0.9858\n",
      "Epoch 18/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0385 - accuracy: 0.9906 - val_loss: 0.0904 - val_accuracy: 0.9849\n",
      "Epoch 19/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0425 - accuracy: 0.9897 - val_loss: 0.0786 - val_accuracy: 0.9867\n",
      "Epoch 20/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0354 - accuracy: 0.9916 - val_loss: 0.0949 - val_accuracy: 0.9854\n",
      "Epoch 21/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0423 - accuracy: 0.9902 - val_loss: 0.1025 - val_accuracy: 0.9827\n",
      "Epoch 22/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0349 - accuracy: 0.9920 - val_loss: 0.1263 - val_accuracy: 0.9820\n",
      "Epoch 23/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0416 - accuracy: 0.9906 - val_loss: 0.0903 - val_accuracy: 0.9880\n",
      "Epoch 24/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0361 - accuracy: 0.9923 - val_loss: 0.1160 - val_accuracy: 0.9870\n",
      "Epoch 25/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0437 - accuracy: 0.9914 - val_loss: 0.1051 - val_accuracy: 0.9859\n",
      "Epoch 26/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0331 - accuracy: 0.9922 - val_loss: 0.1127 - val_accuracy: 0.9878\n",
      "Epoch 27/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0396 - accuracy: 0.9928 - val_loss: 0.1151 - val_accuracy: 0.9867\n",
      "Epoch 28/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0405 - accuracy: 0.9915 - val_loss: 0.1072 - val_accuracy: 0.9876\n",
      "Epoch 29/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0396 - accuracy: 0.9925 - val_loss: 0.1341 - val_accuracy: 0.9863\n",
      "Epoch 30/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0399 - accuracy: 0.9925 - val_loss: 0.1115 - val_accuracy: 0.9864\n",
      "Epoch 31/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0351 - accuracy: 0.9937 - val_loss: 0.1096 - val_accuracy: 0.9850\n",
      "Epoch 32/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0382 - accuracy: 0.9927 - val_loss: 0.1234 - val_accuracy: 0.9861\n",
      "Epoch 33/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0378 - accuracy: 0.9927 - val_loss: 0.1197 - val_accuracy: 0.9882\n",
      "Epoch 34/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0450 - accuracy: 0.9921 - val_loss: 0.1911 - val_accuracy: 0.9804\n",
      "Epoch 35/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0402 - accuracy: 0.9931 - val_loss: 0.1347 - val_accuracy: 0.9860\n",
      "Epoch 36/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0374 - accuracy: 0.9936 - val_loss: 0.1292 - val_accuracy: 0.9883\n",
      "Epoch 37/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0421 - accuracy: 0.9934 - val_loss: 0.1409 - val_accuracy: 0.9870\n",
      "Epoch 38/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0439 - accuracy: 0.9927 - val_loss: 0.1646 - val_accuracy: 0.9854\n",
      "Epoch 39/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0353 - accuracy: 0.9939 - val_loss: 0.1517 - val_accuracy: 0.9879\n",
      "Epoch 40/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0359 - accuracy: 0.9942 - val_loss: 0.1385 - val_accuracy: 0.9884\n",
      "Epoch 41/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0399 - accuracy: 0.9939 - val_loss: 0.1428 - val_accuracy: 0.9874\n",
      "Epoch 42/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0428 - accuracy: 0.9936 - val_loss: 0.1921 - val_accuracy: 0.9856\n",
      "Epoch 43/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0436 - accuracy: 0.9931 - val_loss: 0.1767 - val_accuracy: 0.9867\n",
      "Epoch 44/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0365 - accuracy: 0.9950 - val_loss: 0.1561 - val_accuracy: 0.9867\n",
      "Epoch 45/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0365 - accuracy: 0.9948 - val_loss: 0.2039 - val_accuracy: 0.9871\n",
      "Epoch 46/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0402 - accuracy: 0.9941 - val_loss: 0.2019 - val_accuracy: 0.9864\n",
      "Epoch 47/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0354 - accuracy: 0.9946 - val_loss: 0.1882 - val_accuracy: 0.9878\n",
      "Epoch 48/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0412 - accuracy: 0.9945 - val_loss: 0.2095 - val_accuracy: 0.9871\n",
      "Epoch 49/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0440 - accuracy: 0.9945 - val_loss: 0.2167 - val_accuracy: 0.9872\n",
      "Epoch 50/50\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0435 - accuracy: 0.9943 - val_loss: 0.1978 - val_accuracy: 0.9873\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train_one_hot, epochs=50, validation_split=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f555e-ebbd-427e-acca-2aca47eb572c",
   "metadata": {},
   "source": [
    "The `fit` function executes the training loop, during which the model's parameters are optimized based on the specified optimizer, and the defined loss function is minimized. Throughout each epoch, the training and validation performance metrics are monitored.\r\n",
    "\r\n",
    "The training history, encapsulating valuable information about the model's learning progress, is stored in the `history` variable. This history includes metrics such as training loss, training accuracy, validation loss, and validation accuracy at each epoch. Storing this information allows for later analysis and visualization, enabling a comprehensive understanding of how well the model is learning from the training data and generalizing to unseen validation data.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e685f8c7-ca68-485f-8ceb-5509aa79cf42",
   "metadata": {},
   "source": [
    "\r\n",
    "To optimize the training process, two callbacks, EarlyStopping (`es`) and ModelCheckpoint (`mc`), are incorporated.\r\n",
    "\r\n",
    "- **EarlyStopping (`es`):** Monitors the validation accuracy during training. If the improvement in accuracy is less than the specified `min_delta` for a certain number of `patience` epochs, training is halted. The model's weights are then restored to the best-performing epoch.\r\n",
    "\r\n",
    "- **ModelCheckpoint (`mc`):** Monitors the validation accuracy. Whenever there is an improvement in accuracy, the model's weights are saved to a file named `bestmodel.h5`.\r\n",
    "\r\n",
    "This approach ensures that the training stops early if the model's performance on the validation set does not show significant improvement. Additionally, it saves the weights of the best-performing model, allowing for later use without having to retrain the model from scratch.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30bd2848-b68d-4076-bbbc-510bdd0ae786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1307/1313 [============================>.] - ETA: 0s - loss: 0.0414 - accuracy: 0.9948\n",
      "Epoch 1: val_accuracy did not improve from 0.98811\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0420 - accuracy: 0.9948 - val_loss: 0.2410 - val_accuracy: 0.9880\n",
      "Epoch 2/50\n",
      "1306/1313 [============================>.] - ETA: 0s - loss: 0.0398 - accuracy: 0.9950\n",
      "Epoch 2: val_accuracy did not improve from 0.98811\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0396 - accuracy: 0.9950 - val_loss: 0.2221 - val_accuracy: 0.9870\n",
      "Epoch 3/50\n",
      "1312/1313 [============================>.] - ETA: 0s - loss: 0.0433 - accuracy: 0.9949\n",
      "Epoch 3: val_accuracy improved from 0.98811 to 0.98861, saving model to best.h5\n",
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0433 - accuracy: 0.9949 - val_loss: 0.2204 - val_accuracy: 0.9886\n",
      "Epoch 4/50\n",
      "1306/1313 [============================>.] - ETA: 0s - loss: 0.0527 - accuracy: 0.9945\n",
      "Epoch 4: val_accuracy did not improve from 0.98861\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0525 - accuracy: 0.9945 - val_loss: 0.2158 - val_accuracy: 0.9881\n",
      "Epoch 5/50\n",
      "1312/1313 [============================>.] - ETA: 0s - loss: 0.0482 - accuracy: 0.9942Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.98861\n",
      "1313/1313 [==============================] - 10s 7ms/step - loss: 0.0482 - accuracy: 0.9942 - val_loss: 0.2275 - val_accuracy: 0.9870\n",
      "Epoch 5: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train_one_hot, epochs=50, validation_split=0.3, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496717d6-cae2-4908-b4e3-f42b73098136",
   "metadata": {},
   "source": [
    "The `bestmodel.h5` file serves a crucial purpose. It captures the weights of the model at the epoch where the validation accuracy is the highest. This allows for preserving the best-performing version of the model, avoiding overfitting to the training data. By saving these weights, we can later load the model with its optimal configuration for making predictions on new, unseen data without the need to retrain the entire model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b114ad8-1dc6-4ecd-9ff6-eb3ae7c0422d",
   "metadata": {},
   "source": [
    "## 6. Testing\r\n",
    "\r\n",
    "Now that the model has been trained and the best-performing configuration has been saved in `bestmodel.h5`, let's evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36eb2a97-a41e-403e-ad03-d9d62cf11eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the saved model\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the model in the native Keras format\n",
    "loaded_model = load_model(\"C:\\\\Users\\\\tejus\\\\Desktop\\\\6th Sem Project\\\\MNIST_Model\\\\best.h5\")\n",
    "\n",
    "# Compile the loaded model with the same configuration used during training\n",
    "loaded_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Preprocess the test data similar to the training data\n",
    "X_test_processed = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77e5e466-8f28-4b11-98a3-50cc14470a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2289 - accuracy: 0.9886\n",
      "Test Accuracy: 0.9886000156402588\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_results = saved_model.evaluate(X_test_processed, y_test_one_hot)\n",
    "\n",
    "# Display the key evaluation metrics\n",
    "print(\"Test Accuracy:\", test_results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af4a86-13c3-4019-8137-b84df53cb68c",
   "metadata": {},
   "source": [
    "### 8.Application:\n",
    "This Python code creates an interactive application using the Pygame library. Users can draw numbers on a window with the mouse, and a pre-trained deep learning model (\"bestmodel.h5\") predicts the values in real-time. Pressing the \"N\" key clears the window for a new drawing.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7307c910-c364-46ad-8611-cfe40d6f1304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "# Press \"N\" to clear screen\n",
    "WINDOWSIZEX = 640\n",
    "WINDOWSIZEY = 480\n",
    "\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "RED = (255, 0, 0)\n",
    "\n",
    "MODEL = load_model(\"best.h5\")\n",
    "\n",
    "LABELS = {0: \"Zero\", 1: \"One\",\n",
    "          2: \"Two\", 3: \"Three\",\n",
    "          4: \"Four\", 5: \"Five\",\n",
    "          6: \"Six\", 7: \"Seven\",\n",
    "          8: \"Eight\", 9: \"Nine\"}\n",
    "\n",
    "pygame.init()\n",
    "\n",
    "FONT = pygame.font.Font(\"freesansbold.ttf\", 18)\n",
    "DISPLAYSURF = pygame.display.set_mode((WINDOWSIZEX, WINDOWSIZEY))\n",
    "\n",
    "pygame.display.set_caption(\"Digit Board\")\n",
    "\n",
    "iswriting = False\n",
    "image_cnt = 1\n",
    "BOUNDARYINC = 5\n",
    "PREDICT = True\n",
    "IMAGESAVE = False\n",
    "\n",
    "number_xcord = []\n",
    "number_ycord = []\n",
    "rect_min_x, rect_min_y, rect_max_x, rect_max_y = 0, 0, 0, 0  # Initialize rectangle coordinates\n",
    "\n",
    "while True:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            pygame.quit()\n",
    "            sys.exit()\n",
    "\n",
    "        if event.type == pygame.MOUSEMOTION and iswriting:\n",
    "            xcord, ycord = event.pos\n",
    "            pygame.draw.circle(DISPLAYSURF, WHITE, (xcord, ycord), 4, 0)\n",
    "\n",
    "            number_xcord.append(xcord)\n",
    "            number_ycord.append(ycord)\n",
    "\n",
    "        if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "            iswriting = True\n",
    "\n",
    "        if event.type == pygame.MOUSEBUTTONUP:\n",
    "            iswriting = False\n",
    "            if number_xcord and number_ycord:\n",
    "                rect_min_x, rect_max_x = min(number_xcord) - BOUNDARYINC, max(number_xcord) + BOUNDARYINC\n",
    "                rect_min_y, rect_max_y = min(number_ycord) - BOUNDARYINC, max(number_ycord) + BOUNDARYINC\n",
    "\n",
    "                number_xcord = []\n",
    "                number_ycord = []\n",
    "\n",
    "                image_arr = np.array(pygame.PixelArray(DISPLAYSURF))[rect_min_x:rect_max_x, rect_min_y:rect_max_y].T.astype(np.float32)\n",
    "\n",
    "                if IMAGESAVE:\n",
    "                    cv2.imwrite(f\"image_{image_cnt}.png\", image_arr)\n",
    "                    image_cnt += 1\n",
    "\n",
    "                if PREDICT:\n",
    "                    image = cv2.resize(image_arr, (28, 28))\n",
    "                    image = np.pad(image, ((10, 10), (10, 10)), 'constant', constant_values=0)\n",
    "                    image = cv2.resize(image, (28, 28)) / 255.0\n",
    "\n",
    "                    label = str(LABELS[np.argmax(MODEL.predict(image.reshape(1, 28, 28, 1)))])\n",
    "                    text_surface = FONT.render(label, True, RED, WHITE)\n",
    "                    text_rect_obj = text_surface.get_rect()\n",
    "                    text_rect_obj.left, text_rect_obj.bottom = rect_max_x + 5, rect_min_y\n",
    "\n",
    "                    DISPLAYSURF.blit(text_surface, text_rect_obj)\n",
    "\n",
    "                # Draw a rectangular box around the written number\n",
    "                pygame.draw.rect(DISPLAYSURF, RED, (rect_min_x, rect_min_y, rect_max_x - rect_min_x, rect_max_y - rect_min_y), 2)\n",
    "\n",
    "        if event.type == pygame.KEYDOWN:\n",
    "            if event.unicode == 'n':\n",
    "                DISPLAYSURF.fill(BLACK)\n",
    "\n",
    "    pygame.display.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753fbaaa-de47-436e-84fd-515fe53c5a09",
   "metadata": {},
   "source": [
    "## Project Conclusion\n",
    "\n",
    "In this project, we followed a structured workflow to build and train a Convolutional Neural Network (CNN) using Keras for the classification of MNIST handwritten numerical digits. Let's summarize our key findings and achievements based on the project workflow:\n",
    "\n",
    "### Project Workflow:\n",
    "\n",
    "1. **Data Loading and Exploration:** We successfully loaded and explored the MNIST dataset, gaining insights into the structure of the training and testing data.\n",
    "\n",
    "2. **Data Preprocessing:** Prior to feeding the data into the model, we performed essential preprocessing steps, including converting integer labels to one-hot encoding and reshaping the input data to meet the requirements of a CNN.\n",
    "\n",
    "3. **Model Architecture:** The CNN architecture was meticulously designed, comprising convolutional layers, max-pooling layers, flattening, dropout for regularization, and a densely connected output layer tailored for digit classification.\n",
    "\n",
    "4. **Compilation:** The model was compiled using the Adam optimizer, categorical crossentropy loss function, and accuracy as the monitoring metric. \n",
    "\n",
    "5. **Training:** Training was executed for 50 epochs, with a validation split of 30%. EarlyStopping and ModelCheckpoint callbacks were employed to enhance training efficiency and preserve the best-performing model.\n",
    "   \n",
    "7.  **Testing:** The testing phase ensures the loaded model's accurate performance on new data, addressing warnings by manually compiling, and evaluating key metrics for insights into its effectiveness.\n",
    "\n",
    "\n",
    "8.  **Application:** This Python code utilizes the Pygame library to create an interactive application where users can draw numbers on a window using the mouse. The drawn digits are then processed by a pre-trained deep learning model (loaded from \"bestmodel.h5\"), and the predicted values are displayed alongside the drawn numbers in real-time. \n",
    "\n",
    "\n",
    "### Future Considerations:\n",
    "\n",
    "As we conclude this project, there are avenues for further exploration, such as hyperparameter tuning, experimenting with different architectures, or exploring transfer learning approaches for enhanced performance.\n",
    "\n",
    "This project not only provided practical insights into building and training CNNs but also emphasized the significance of thoughtful model architecture, training strategies, and the use of callbacks to improve overall model performance.\n",
    "rall model performance.\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
